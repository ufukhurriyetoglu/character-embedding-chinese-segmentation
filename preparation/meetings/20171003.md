_20171003_

[Learning Character-level Compositionality with Visual Features](https://arxiv.org/abs/1704.04859)

- Hypothesis
  - CNN will focus on parts of the image that contain semantic information useful for category classification
  - If more robust to infrequent characters, may perform better in low-resourced scenarios
- Advantage
  - Simple
  - Generalizable to different languages
  - Handle rare characters effectively
  - Complementary with standard character embeddings
  - ~Effective under low-resource scenarios~
- Disadvantage
  - Noise for characters visually similar but having different meanings
  - 30x more training time
  - More storage
- Dataset
  - Requirements
    - Must fully utilize each character in the input
    - Must be enough compositionality & regularity in the language
  - Input
    - Wikipedia article title in Chinese, Japanese, or Korean
  - Output
    - Category the article belongs to
  - Statistics
    - Each language follows 80/20 rule for character rank-frequency distribution (long-tail distribution)
    - Training:validation:testing = 6:2:2
    - Traditional Chinese & simplified Chinese
- Model
  - Visual model
    - CNN: calculate character representation
    - RNN (GRU): combine character representations into sentence representation
    - Softmax: probability distribution for each class
  - Lookup model
    - Character embeddings from lookup matrix
    - Others same as visual model
- Fusion
  - Early fusion
    - Concatenate visual & lookup character embeddings & fed through a hidden layer to reduce dimension
  - Late fusion
    - Averages output probabilities from both modelst
  - Fallback fusion
    - Visual model for instances with average character frequency <= threshold; Lookup model for others
- Evaluations
  - Compare __image input__ against __symbol input__
  - Train on less data
    - Didn't perform well
    - CNN cannot learn features from insufficient data
  - Use the trained model of simplified Chinese to test on traditional Chinese
    - `40%` accuracy (v.s. `55%`)
    - Models can transfer between similar scripts
  - Fusion methods
    - Better performance than individual models -> Visual & Lookup capture complementary features
  - Character embeddings
    - Emphasis of Visual model & K-nearest neighbors
      - Lookup model does not capture semantic meanings
        - Category classification task not relying on character semantics
        - Dataset contains many names & locations

The paper tries to learn character embeddings from character images with CNN. Constituents that contribute more to a character's overall meaning, e.g. radicals (部首), tend to be captured, since characters with the same radicals should have similar meanings.

The evaluation test is to predict the category of a given Wikipedia title. The visual model proposed was compared with a standard lookup model. These two models were fed into RNN (GRU) to learn the overall title embedding, and prediction on the title's category was made on the probability distribution produced. The result shows that this visual model performs better on titles with rare-characters, but worse in other cases. This is probably due to the fact that the visual model can guess meanings better from constituent information of a character.

A summary on the advantages & disadvantages of this visual model:

- Advantage
  - Simple
  - Generalizable to different languages
  - Handle rare characters effectively
  - Complementary with standard character embeddings (a fusion of the two models enhances the overall performance)
- Disadvantage
  - Noise for characters visually similar but having different meanings
  - 30x more training time
  - More storage
