_20171003_

[What is the model]  
[Fusion methods]

- Hypothesis
  - CNN will focus on parts of the image that contain semantic information useful for category classification
 Â - If more robust to infrequent characters, may perform better in low-resourced scenarios
- Advantage
  - Simple
  - Generalizable to different languages
  - Handle rare characters effectively
  - Effective under low-resource scenarios
- Disadvantage
  - Noise for characters visually similar but having different meanings
  - 30x more training time
  - More storage
- Dataset
  - Requirements
    - Must fully utilize each character in the input
    - Must be enough compositionality & regularity in the language
  - Input
    - Wikipedia article title in Chinese, Japanese, or Korean
  - Output
    - Category the article belongs to
  - Statistics
    - Each language follows 80/20 rule for character rank-frequency distribution (long-tail distribution)
    - Training:validation:testing = 6:2:2
    - Traditional Chinese & simplified Chinese
- Model
  - Visual model
    - CNN: calculate character representation
    - RNN (GRU): combine character representations into sentence representation
    - Softmax: probability distribution for each class
  - Lookup model
    - Character embeddings from lookup matrix
    - Others same as visual model
- Fusion
  - Early fusion
    - Concatenate visual & lookup character embeddings & fed through a hidden layer to reduce dimension
  - Late fusion
    - Averages output probabilities from both models
  - Fallback fusion
    - Visual model for instances with average character frequency <= threshold; Lookup model for others
- Evaluations
  - Compare __image input__ against __symbol input__

[Possible directions]
